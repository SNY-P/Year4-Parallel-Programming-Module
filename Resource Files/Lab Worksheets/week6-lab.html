<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link href="./week6-lab_files/mystyle.css" rel="stylesheet" type="text/css">
</head>

<body style="" data-new-gr-c-s-check-loaded="14.1111.0" data-gr-ext-installed="">

<div id="container">

<div id="header">
<h1>Parallel Programming Lab, Week 6:<br>MPJ Communication</h1>

</div>

<div id="content">

<p>
The main goal of last week's lab was to get MPJ programs running at all
in our cluster environment.  This week, assuming you succeeded in that,
I want you to concentrate on
one particular - more complex - distributed-memory parallel program.</p>

<p>
Up till now in these labs, parallel speedup should, on the whole,
have come relatively easily.  The program you will study this week involves
a considerable amount of inter-process communication, and speedup is a
harder proposition.  You should hopefully see speedup if you run
your program in the MPJ Express "multicore mode".  I also hope you will
be able to demonstrate <i>some</i> speedup in "cluster mode".
Ultimately, though, our problems remains small - parallel computing
works best for really large problems.</p>

<p>
First, please read through the week 5 lecture about data decomposition
and MPI communication.  That lecture describes how to make an MPI
parallel version of Conway's Game of Life.  In this lab I will ask
you to experiment instead with an MPJ parallel version of our Laplace
Equation solver.  But it follows a very similar coding pattern.</p>

<h2>A Distributed Memory Solver</h2>

<p>
We follow the prescription given in the lecture for Life
very closely, adding <i>ghost regions</i> and <i>edge swap</i>
communications.
In detail, the Laplace equation is slightly more complicated because
we have to worry about how the fixed boundary conditions at the edges of
the system interact with the ghost regions.</p>

<p>
In the sequential algorithm the array was organized like this:
</p>
<p>
<img src="./week6-lab_files/LaplaceSequential.jpg" height="400">
</p>
<p>
The yellow border represents array elements where the field <tt>phi</tt>
is set to fixed boundary values.  The central green area represents
array elements
that will be updated in the algorithm.  The numbers at the side of the grid
represent the ranges of the indexes used in the update loops.</p>

<p>
When we decompose this in the <i>i</i> dimension, and add ghost regions
at the edges of the blocks, we get something like this:</p>
<p>
<img src="./week6-lab_files/LaplaceDistributed.jpg" height="440">
</p>
<p>
Here the blue areas are the ghost regions.  <i>B</i> is the unextended
block size, <i>N</i>/<i>P</i>.  With ghost regions added, the local arrays
are all <span class="eqn">(<i>B</i>+2) × <i>N</i></span>.  The range of indexes in these arrays
that appear in the update loops is also shown.  Generally <i>i</i> loops
start at 1 and ends at <i>B</i>, avoiding the ghost regions.  But because
of the fixed boundary conditions, this range has to be further restricted
for process 0 and process <span class="eqn"><i>P</i>-1</span>.</p>

<!--
<p>(<i>A confession</i>: Between lecture and lab script, I have switched my
convention on vertical and horizontal axes in pictures like the one above.
In the week 17 lecture, I followed the mathematical convention of treating
the two-dimensional arrays like matrices, so "<i>i</i>" is vertical
and "<i>j</i>" is horizontal.  In the labs I have followed the graphics
convention, so <i>i</i> horizontal and <i>j</i> is vertical.  This
<i>is</i> only a convention on how pictures are drawn - please don't
let it confuse you.  Generally speaking one should choose one convention
and stick with it!)</p>
-->

<p>
Putting together the various ingredients above gives us the following
fairly long, MPJ-based, distributed memory, version of the Laplace Equation:
</p><pre>  import mpi.* ;
  
  import java.awt.* ;
  import javax.swing.* ;
  
  public class MPJLaplace {
      
    final static int N = 256 ;
    final static int CELL_SIZE = 2 ;
    final static int NITER = 100000 ;
    final static int OUTPUT_FREQ = 1 ;
  
    static int P, me, B ;
	  
    static float [][] phi ;
    static float [][] newPhi ;
  
    static float [][] allPhi ;  // temporary array for display
	  
    static Display display ;
  
    public static void main(String args []) throws Exception {
	  
      MPI.Init(args) ;
		  
      me = MPI.COMM_WORLD.Rank() ;
      P = MPI.COMM_WORLD.Size() ;
		
      if(me == 0) {  
        allPhi = new float [N] [N] ;  // used for display only
        display = new Display() ;
      }
		  
      B = N / P ;
<span style="color:red">
      phi    = new float [B+2][N] ;
      newPhi = new float [B+2][N] ;
</span>
      // Make voltage non-zero on left and right edges
  
      if(me == 0) {
        for(int j = 0 ; j &lt; N ; j++) {
          phi [1] [j] = 1.0F ;
        }
      }

      if(me == P-1) {
        for(int j = 0 ; j &lt; N ; j++) {
          phi [B] [j] = 1.0F ;
        }
      }
  
      displayPhi() ;
<span style="color:red">
      // Don't change ghost regions in update loops

      int begin = 1 ;
      int end = B + 1 ;
		  
      // Don't update fixed boundary values

      if(me == 0) {
        begin = 2 ;
      }

      if (me == P-1) {
        end = B ;
      }
</span>
      // Main update loop.
  
      long startTime = System.currentTimeMillis();
  
      for(int iter = 0 ; iter &lt; NITER ; iter++) {
<span style="color:red">
        // Edge swap
			  
        int next = (me + 1) % P ;
        int prev = (me - 1 + P) % P ;
        MPI.COMM_WORLD.Sendrecv(phi [B], 0, N, MPI.FLOAT, next, 0,
                                phi [0], 0, N, MPI.FLOAT, prev, 0) ;
        MPI.COMM_WORLD.Sendrecv(phi [1], 0, N, MPI.FLOAT, prev, 0,
                                phi [B+1], 0, N, MPI.FLOAT, next, 0) ;
</span>
        // Calculate new phi

        for(int i = <span style="color:red">begin</span> ; i &lt; <span style="color:red">end</span> ; i++) {
          for(int j = 1 ; j &lt; N - 1 ; j++) {
  
            newPhi [i] [j] =
                    0.25F * (phi [i] [j - 1] + phi [i] [j + 1] +
                             phi [i - 1] [j] + phi [i + 1] [j]) ;
          }
        }
  
        // Update all phi values
      
        for(int i = <span style="color:red">begin</span> ; i &lt; <span style="color:red">end</span> ; i++) {
          for(int j = 1 ; j &lt; N - 1 ; j++) {
            phi [i] [j] = newPhi [i] [j] ;
          }
        }
      
        if(iter % OUTPUT_FREQ == 0) {
          if(me == 0) {
            System.out.println("iter = " + iter) ;
          }
          displayPhi() ;
        }
      }
  
      long endTime = System.currentTimeMillis();
  
      if(me == 0) {
        System.out.println("Calculation completed in " +
                           (endTime - startTime) + " milliseconds");
      }
  
      displayPhi() ;

      MPI.Finalize() ;
    }
	  
    public static void displayPhi() {
          
      if(me &gt; 0) {
        MPI.COMM_WORLD.Send(phi, 1, B, MPI.OBJECT, 0, 0) ;
      }
      else {  // me == 0
        for(int i = 1 ; i &lt;= B ; i++) {
          for(int j = 0 ; j &lt; N ; j++) {
            allPhi [i - 1] [j] = phi [i] [j] ;
          }
        }
        for(int src = 1 ; src &lt; P ; src++) {
          MPI.COMM_WORLD.Recv(allPhi, src * B, B, MPI.OBJECT, src, 0) ;
        }
			  
        display.repaint() ;
      }
    }
  
    static class Display extends JPanel {
  
      final static int WINDOW_SIZE = N * CELL_SIZE ;
  
      Display() {
        setPreferredSize(new Dimension(WINDOW_SIZE, WINDOW_SIZE)) ;
  
        JFrame frame = new JFrame("Laplace");
        frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);
        frame.setContentPane(this);
        frame.pack();
        frame.setVisible(true);
      }
  
      public void paintComponent(Graphics g) {
        for(int i = 0 ; i &lt; N ; i++) {
          for(int j = 0 ; j &lt; N ; j++) {
            float f = allPhi [i] [j] ;
            Color c = new Color(f, 0.0F, 1.0F - f) ;
            g.setColor(c) ;
            g.fillRect(CELL_SIZE * i, CELL_SIZE * j,
                       CELL_SIZE, CELL_SIZE) ;
          }
        } 
      }
    }
  }
</pre>
The parts of the code primarily related to dealing with edges (including
edge swap) are highlighted in red.  These are the parts you should
particularly try to understand.<p></p>

<p>
A few comments on this code:
</p><ul>
<li>
You may notice that the ghost regions at the extreme edges (left of process
zero and right of process <span class="eqn"><i>P</i>-1</span>) are redundant.  The values they contain
are never used.  They are included for uniformity, to make the code
a little simpler.
With some extra work redundant communications can be avoided - see below.
</li>
<li>
I have put ghost regions on the <tt>newPhi</tt> array as well as the
<tt>phi</tt> array.  These aren't really needed and they are slightly
wasteful of memory.  But they make subscripting of the arrays a little
more uniform.
</li>
<li>
The array <tt>allPhi</tt> has a role identical to the array <tt>allSet</tt>
in the discussion of the distributed Mandelbrot Set also in the week 17 lecture.
In the method <tt>displayPhi()</tt> it is used to <i>gather</i> the data
from the distributed blocks (excluding ghost regions), ready for display
by process zero.
</li>
</ul>

<h2>Experiments and Questions</h2>

<p>
Last week, to connect to the cluster, I suggested you use PuTTY.
PuTTY remains a good lightweight solution if you just need a remote
terminal.  But this week's main example
generates graphical output, so you also need an X Server running on
your local machine.</p>

<h3>Windows users</h3>

<p>
I experimented with few of open source standalone X servers for
Windows, but so far the most robust (free) solution I have found is
<a href="https://mobaxterm.mobatek.net/">MobaXterm</a> from Mobatek.
It isn't open source, but there is a free home edition available.
It integrates a remote terminal with a built in X server for graphics,
and also has a slightly more modern and user-friendly interface than
PuTTY.</p>

<p>
If you are working on a Windows home computer, I suggest you download
and install the home edition of MobaXterm.</p>

<p>
In the "Session" tab of MobaXterm, select "SSH" and set "Remote host"
to mn01.soc.uni.ds.port.ac.uk - for me that was as much as I needed to
do to give a terminal to the Hadoop head node, with graphics
transparently tunnelled to my laptop and displayed.</p>

<h3>Mac or Linux users</h3>

<p>
If your home computer is a Mac, you <i>may</i> need to first install
the XQuartz X11 server, if it isn't already on your system.

</p><p>
From a Mac or Linux, you should
be able to ssh to the cluster by opening a terminal and running a command
similar to:
</p><pre>  $ ssh -Y dbc@mn01.soc.uni.ds.port.ac.uk
</pre>
where <tt>dbc</tt> is my account ID on the cluster.<p></p>

<p>
(In some cases it may be necessary to issue a command similar to:
</p><pre>  $ export DISPLAY=localhost:0.0
</pre>
before using <tt>ssh -Y</tt>.  On some systems, <tt>ssh -X</tt> may
work instead.  You may need to do some experimentation.<p></p>

<p>
A good test of whether X11 forwarding is working correctly from the cluster
is to issue a command like:
</p><pre>  $ xclock
</pre>
on mn01.soc.
If this does <i>not</i> create a clock window on your home computer, check
the local value of the DISPLAY environment variable on mn01.soc by:
<pre>  $ echo $DISPLAY
</pre>
If this variable is empty, experiment with different ways of running
ssh from your home computer, as described above.)
<p>

</p><h3>Running the code</h3>

<p>
Now, following instructions from last week, compile and run the MPJLaplace
code given above.  To compile on mn01.soc:
</p><pre>  $ javac MPJLaplace.java
</pre>
To run in multicore mode, e.g.:
<pre>  $ mpjrun.sh -np 2 MPJLaplace
</pre>
<p></p>

<p>
Run <tt>MPJLaplace</tt> in <i>multicore</i>
mode with <i>P</i> equal to 1, 2, 4, etc.
<i>Before recording timings</i>, make sure you increase <tt>OUTPUT_FREQ</tt>
to, say, 1000, so that output overheads don't dominate the results.</p>

<p>
Compare the results with those
obtained using the sequential version of <tt>Laplace</tt> given in
week 4.  If you also developed a shared memory parallel version of
the Laplace solver that week, compare your results for <tt>MPJLaplace</tt>
in multicore mode with the ones you obtained previously.</p>

<h3>Running across multiple nodes</h3>

<p>
A pre-requisite is that <tt>mpjdaemon</tt> is running on two or more
nodes in the cluster, and that you have set up your <tt>machines</tt>
file to refer to these daemons, as described last week.  For this week
some extra considerations apply if you want to be able to see the
graphics output of the program while running in cluster mode.</p>

<p>
In general you can share use of MPJ Daemons started by other users.
By default there can only be one daemon running on each node.  So,
especially during timetabled practial sessions, some sharing of daemons may be a
necessity, because there are a limited number of nodes available.</p>

<p>
<i>But</i>, to be able to get graphics back from
the first process of your MPJ program (<tt>me == 0</tt>), the <i>first</i>
daemon node named in your <tt>machines</tt> file <i>should have been started
by you personally</i>, with X forwarding to your home computer enabled.
To start <i>that</i>
daemon you should remote to mn01.soc as described above, then from there
use <tt>ssh -Y</tt> to remote to the node where you want to run your
own daemon, e.g.:
</p><pre>  $ ssh -Y wn04.soc
</pre>
(specifiying a chosen, free node).  In this example I would then make
sure wn04.soc was the first host appearing in my <tt>machines</tt> file.<p></p>

<p>
Now, try to run <tt>MPJLaplace</tt> across two daemons like this:
</p><pre>  $ mpjrun.sh -np 2 -dev niodev -headnodeip mn01.soc MPJLaplace
</pre>
Again, we assume you increased <tt>OUTPUT_FREQ</tt>
to, say, 1000, so that output overheads don't dominate the results.<p></p>

<p>
I have to report that, as things stand, running on two nodes in cluster mode
didn't produce a parallel speedup for me.  Don't be immediately discouraged!</p>

<p>
Estimate the number of communications
in the program - these should be the main overheads inhibiting speed gain.</p>

<p>
Try to measure the cost of communication by temporarily commenting out the
section of code called "Edge swap" and see what happens to timings.
When you are done with this, reinstate the edge swap code.</p>

<p>
As mentioned above, the ghost regions at left of process
zero and right of process <span class="eqn"><i>P</i>-1</span> are redundant.
To avoid the overhead of updating them replace the edge swap code with:
</p><pre>        int next = me + 1 ;
        int prev = me - 1 ;
        if(prev &gt;= 0 &amp;&amp; next &lt; P) {
          MPI.COMM_WORLD.Sendrecv(phi [B], 0, N, MPI.FLOAT, next, 0,
                                  phi [0], 0, N, MPI.FLOAT, prev, 0) ;
          MPI.COMM_WORLD.Sendrecv(phi [1], 0, N, MPI.FLOAT, prev, 0,
                                  phi [B+1], 0, N, MPI.FLOAT, next, 0) ;
        }
        else if(prev &gt;= 0) {  // next == P
          MPI.COMM_WORLD.Recv(phi [0], 0, N, MPI.FLOAT, prev, 0) ;
          MPI.COMM_WORLD.Send(phi [1], 0, N, MPI.FLOAT, prev, 0) ;
        }
        else if(next &lt; P) {  // prev == -1
          MPI.COMM_WORLD.Send(phi [B], 0, N, MPI.FLOAT, next, 0) ;
          MPI.COMM_WORLD.Recv(phi [B+1], 0, N, MPI.FLOAT, next, 0) ;
        }
</pre>
Does it make any difference (in theory it should halve the total amount
of communication for <span class="eqn"><i>P</i> = 2</span> in particular)?
<p></p>

<p>
Try running <tt>MPJLaplace</tt> in cluster mode with 4, 8, etc processes.
If this produces parallel speedup, can you explain why?</p>

<p>
Now increase the problem size.  Change <i>N</i> to 512 and repeat your
timings.  To save excessive waiting you will probably want to reduce
<tt>NITER</tt> to say 10000.</p> 

<p>
Change <i>N</i> to 1024 (possibly
reduce CELL_SIZE to 1 so that the graphics window fits on your screen, and reduce NITER again if you like)
and repeat the timings.  By now you <i>should</i> be seeing at least modest parallel
speedups in cluster mode.</p>

<p>
Why does increasing the size of a problem generally make it more amenable to
parallel computation?</p>

<p>
After the lecture on collective communications, try to recode the method
<tt>displayPhi()</tt> using the <tt>Gather()</tt> collective operation (see
for example slide 22).</p>

</div>

<div id="footer">
Copyright © University of Portsmouth, 2020
</div>




</div></body><grammarly-desktop-integration data-grammarly-shadow-root="true"><template shadowrootmode="open"><style>
  div.grammarly-desktop-integration {
    position: absolute;
    width: 1px;
    height: 1px;
    padding: 0;
    margin: -1px;
    overflow: hidden;
    clip: rect(0, 0, 0, 0);
    white-space: nowrap;
    border: 0;
    -moz-user-select: none;
    -webkit-user-select: none;
    -ms-user-select:none;
    user-select:none;
  }

  div.grammarly-desktop-integration:before {
    content: attr(data-content);
  }
</style><div aria-label="grammarly-integration" role="group" tabindex="-1" class="grammarly-desktop-integration" data-content="{&quot;mode&quot;:&quot;full&quot;,&quot;isActive&quot;:true,&quot;isUserDisabled&quot;:false}"></div></template></grammarly-desktop-integration></html>